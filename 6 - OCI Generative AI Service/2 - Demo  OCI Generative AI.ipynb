{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616976c9",
   "metadata": {},
   "source": [
    "# **Lesson: Demo of OCI Generative AI Service**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objective**\n",
    "\n",
    "In this demo, you will learn how to navigate, access, and use the **Oracle Cloud Infrastructure (OCI) Generative AI Service** through the OCI Console.  \n",
    "You will explore its main interface, the **Playground**, and understand how to create dedicated clusters, fine-tuned models, and endpoints for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Welcome to this **demo of the OCI Generative AI Service**.  \n",
    "In this session, we’ll walk through the OCI Console and demonstrate how to use the Generative AI dashboard, explore pre-trained models, and generate code for integration with applications.  \n",
    "\n",
    "For this demo, we are logged into the **OCI Console**, specifically in the **Germany Central (Frankfurt)** region — one of the currently supported regions for the Generative AI service.  \n",
    "Ensure that the service is available in your selected region before proceeding.\n",
    "\n",
    "---\n",
    "\n",
    "## **Navigating to the OCI Generative AI Service**\n",
    "\n",
    "1. From the **OCI Console**, click the **burger menu (≡)** on the top left.  \n",
    "2. Select **Analytics & AI** from the menu.  \n",
    "3. Under **AI Services**, click **Generative AI**.  \n",
    "\n",
    "This will take you to the **Generative AI Dashboard**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exploring the Generative AI Dashboard**\n",
    "\n",
    "The dashboard provides access to several components:\n",
    "\n",
    "- **Service Tour:** A quick introduction video explaining service features.  \n",
    "- **Documentation:** Links to detailed API references and model information.  \n",
    "- **Playground:** A no-code visual interface to test and explore models.\n",
    "\n",
    "You will also see sections for:\n",
    "\n",
    "- **Dedicated AI Clusters:** GPU-based compute resources for fine-tuning and hosting models.  \n",
    "- **Custom Models:** Fine-tuned versions of base models.  \n",
    "- **Endpoints:** Hosting points for serving inference traffic.  \n",
    "\n",
    "Initially, these will be empty until you create clusters or fine-tuned models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Using the Playground**\n",
    "\n",
    "Click on **Playground** to open the interactive interface.  \n",
    "On the left-hand side, you’ll find two categories of **pre-trained foundational models**:\n",
    "\n",
    "- **Chat Models**\n",
    "- **Embedding Models**\n",
    "\n",
    "### **1. Chat Models**\n",
    "\n",
    "Under the *Chat* section, you’ll see available models:\n",
    "\n",
    "- **Command-R**\n",
    "- **Command-R-Plus**\n",
    "- **Meta Llama 3 – 70B Instruct**\n",
    "\n",
    "You can read detailed descriptions by clicking **Model Details** or following the documentation links.\n",
    "\n",
    "**Token Limits Comparison:**\n",
    "| Model | Token Limit | Use Case |\n",
    "|--------|--------------|----------|\n",
    "| **Command-R-Plus** | 128,000 tokens | High-end applications |\n",
    "| **Command-R** | 16,000 tokens | General-purpose use |\n",
    "| **Llama 3 (70B)** | 8,000 tokens | Lightweight inference |\n",
    "\n",
    "---\n",
    "\n",
    "### **Interacting with Chat Models**\n",
    "\n",
    "The chat interface retains **context**, allowing for follow-up questions.  \n",
    "For example:\n",
    "- Prompt: *“Teach me how to fish.”*  \n",
    "  → Model outputs detailed steps.\n",
    "- Follow-up: *“Describe step 3.”*  \n",
    "  → The model recalls that step 3 was “choosing a location” and elaborates on it.\n",
    "\n",
    "This demonstrates **contextual continuity**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Viewing and Using Generated Code**\n",
    "\n",
    "Once satisfied with the response:\n",
    "- Click **View Code**.  \n",
    "- Choose your preferred language (Python or Java).  \n",
    "- The console displays:\n",
    "  - The API client setup.  \n",
    "  - Sample inference code.  \n",
    "  - Parameters and authentication details.  \n",
    "\n",
    "You can **copy** this code and run it in your IDE or Jupyter Notebook to integrate OCI Generative AI directly into your application.\n",
    "\n",
    "---\n",
    "\n",
    "### **Adjusting Model Parameters**\n",
    "\n",
    "If you’re not satisfied with the model’s tone or behavior, you can modify parameters such as:\n",
    "\n",
    "- **Preamble Override:** Defines the model’s persona or style.  \n",
    "  - Example: Set it to *“You are a travel advisor who speaks like a pirate.”*  \n",
    "  - Result: The output adopts a pirate-style tone.  \n",
    "\n",
    "- **Temperature:** Controls output randomness.  \n",
    "  - Lower values = more deterministic results.  \n",
    "  - Higher values = more creative and varied output.\n",
    "\n",
    "These controls allow dynamic experimentation without retraining or fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Embedding Models**\n",
    "\n",
    "Click on **Embedding** from the left panel to explore models for semantic search and vector representation.\n",
    "\n",
    "**Available Models:**\n",
    "- **Embed-English**\n",
    "- **Embed-Multilingual**\n",
    "\n",
    "### **Example: HR Help Center Articles**\n",
    "\n",
    "By running the **HR Help Center** example:\n",
    "- 41 article titles are converted into **vector embeddings**.\n",
    "- Each text becomes a point in a high-dimensional space (e.g., 384 dimensions).\n",
    "- The playground visualizes them in 2D clusters.\n",
    "\n",
    "Articles with **similar meanings** (e.g., \"learning skills\" or \"vacation policies\") appear close together — demonstrating **semantic similarity**.\n",
    "\n",
    "This is the foundation of **semantic search**, where search focuses on *meaning* rather than exact keywords (lexical search).\n",
    "\n",
    "Like before, you can click **View Code** to see the Python or Java example for embedding generation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Creating Dedicated AI Clusters**\n",
    "\n",
    "Dedicated AI Clusters are **GPU-based compute environments** for fine-tuning and inference.\n",
    "\n",
    "To create one:\n",
    "1. Click **Create Dedicated AI Cluster**.  \n",
    "2. Assign a **name**.  \n",
    "3. Select the **cluster purpose**:\n",
    "   - **Fine-tuning**\n",
    "   - **Inference hosting**\n",
    "4. Choose a **pre-trained model**.\n",
    "5. Click **Create** to deploy your cluster.\n",
    "\n",
    "These clusters provide **isolated GPU resources** and **low-latency networking** for optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Creating Fine-tuned (Custom) Models**\n",
    "\n",
    "Fine-tuning allows you to adapt foundational models to your domain.\n",
    "\n",
    "Steps to create:\n",
    "1. Click **Create Model**.  \n",
    "2. Enter a **model name**.  \n",
    "3. Select a **base model**.  \n",
    "4. Choose a **fine-tuning method** (such as *T-Few Fine-tuning*).  \n",
    "5. If needed, create a new dedicated AI cluster during setup.  \n",
    "\n",
    "Once fine-tuned, your custom model can be hosted for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## **Creating Endpoints**\n",
    "\n",
    "Endpoints allow your applications to access fine-tuned models for real-time inference.\n",
    "\n",
    "To create one:\n",
    "1. Click **Create Endpoint**.  \n",
    "2. Enter a **name** and **hosting configuration**.  \n",
    "3. Select your **fine-tuned model**.  \n",
    "4. Attach a **dedicated AI cluster**.  \n",
    "\n",
    "Once deployed, your endpoint can serve live inference requests.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "| Component | Description |\n",
    "|------------|--------------|\n",
    "| **Playground** | Interactive tool to test chat and embedding models. |\n",
    "| **Chat Models** | Generate natural dialogue and text output. |\n",
    "| **Embedding Models** | Convert text to vectors for semantic search. |\n",
    "| **Dedicated AI Clusters** | GPU-based compute for fine-tuning and inference. |\n",
    "| **Custom Models** | Fine-tuned models specialized for specific tasks. |\n",
    "| **Endpoints** | Host models for production inference. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "This demo showcased how to:\n",
    "- Navigate the **OCI Generative AI Console**.  \n",
    "- Use the **Playground** to experiment with chat and embedding models.  \n",
    "- Generate code directly for integration.  \n",
    "- Create **dedicated clusters**, **custom models**, and **endpoints**.  \n",
    "\n",
    "**Key Takeaway:**  \n",
    "> OCI Generative AI Service offers a complete ecosystem for building, customizing, and deploying generative AI models — all from a single, unified console.\n",
    "\n",
    "**End of Demo**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
